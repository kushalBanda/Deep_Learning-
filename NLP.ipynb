{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to NLP Fundamentals in TensorFlow\n",
    "\n",
    "* NLP has the goal of deriving information out of natural language (could be sequences text or speech).\n",
    "\n",
    "* Another common term for NLP problems is sequence to sequence problems (seq2seq)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN:** A recurrent neural network (RNN) is a class of ANN where connections between nodes form a directed graph along a temporal sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wget\n",
    "# wget.download(url = \"https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import unzip_data , create_tensorboard_callback , plot_loss_curves, compare_historys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unzip_data(\"nlp_getting_started.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2644</th>\n",
       "      <td>3796</td>\n",
       "      <td>destruction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>So you have a new weapon that can cause un-ima...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>3185</td>\n",
       "      <td>deluge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The f$&amp;amp;@ing things I do for #GISHWHES Just...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5448</th>\n",
       "      <td>7769</td>\n",
       "      <td>police</td>\n",
       "      <td>UK</td>\n",
       "      <td>DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>191</td>\n",
       "      <td>aftershock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Aftershock back to school kick off was great. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6845</th>\n",
       "      <td>9810</td>\n",
       "      <td>trauma</td>\n",
       "      <td>Montgomery County, MD</td>\n",
       "      <td>in response to trauma Children of Addicts deve...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id      keyword               location  \\\n",
       "2644  3796  destruction                    NaN   \n",
       "2227  3185       deluge                    NaN   \n",
       "5448  7769       police                     UK   \n",
       "132    191   aftershock                    NaN   \n",
       "6845  9810       trauma  Montgomery County, MD   \n",
       "\n",
       "                                                   text  target  \n",
       "2644  So you have a new weapon that can cause un-ima...       1  \n",
       "2227  The f$&amp;@ing things I do for #GISHWHES Just...       0  \n",
       "5448  DT @georgegalloway: RT @Galloway4Mayor: ÛÏThe...       1  \n",
       "132   Aftershock back to school kick off was great. ...       0  \n",
       "6845  in response to trauma Children of Addicts deve...       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle training dataframe\n",
    "train_df_shuffled = train_df.sample(frac = 1, random_state = 42)\n",
    "train_df_shuffled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What does the test dataframe look like?\n",
    "test_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many example of each class?\n",
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 3263)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many total samples?\n",
    "len(train_df) , len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Latest : Trains derailment: 'It's the freakiest of freak accidents' - The Indian Express: The Indi... http://t.co/iLdbeJe225 #IndianNews\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Motorcyclist bicyclist injured in Denver collision on Broadway: http://t.co/241cN8yxjq by @kierannicholson\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "gmtTy mhtw4fnet\n",
      "\n",
      "Officials: Alabama home quarantined over possible Ebola case - Washington Times\n",
      "\n",
      "---\n",
      "\n",
      "Target: 0 (not real disaster)\n",
      "Text:\n",
      "Your Router is One of the Latest DDoS Attack Weapons http://t.co/vXxMvgtzvg #phone #gaming #tv #news\n",
      "\n",
      "---\n",
      "\n",
      "Target: 1 (real disaster)\n",
      "Text:\n",
      "Ignition Knock (Detonation) Sensor-Senso Standard KS94 http://t.co/IhphZCkm41 http://t.co/wuICdTTUhf\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's visualize some random training examples\n",
    "import random \n",
    "random_index = random.randint(0 , len(train_df) - 5)\n",
    "for row in train_df_shuffled[['text' , 'target']][random_index: random_index + 5].itertuples():\n",
    "    _, text , target = row\n",
    "    print(f\"Target: {target}\", \"(real disaster)\" if target > 0 else \"(not real disaster)\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and validation sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "train_sentences , val_sentences , train_labels , val_labels = tts(train_df_shuffled['text'].to_numpy(),\n",
    "                                                                    train_df_shuffled['target'].to_numpy(),\n",
    "                                                                    test_size = 0.1, # use 10% of training data for validation\n",
    "                                                                    random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6851, 762, 6851, 762)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the lengths\n",
    "len(train_sentences) , len(val_sentences) , len(train_labels) , len(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
       "        'Imagine getting flattened by Kurt Zouma',\n",
       "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
       "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
       "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
       "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
       "        'destroy the free fandom honestly',\n",
       "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
       "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
       "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
       "       dtype=object),\n",
       " array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first 10 samples\n",
    "train_sentences[:10] , train_labels[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Tokenization** - Straight mapping from token to number (can be modelled but quickly gets too big).\n",
    "* **Embedding** - Richer representation of relationships between tokens (can limit size + can be learned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, there are two main concepts for turning text into numbers:\n",
    "\n",
    "* Tokenization - A straight mapping from word or character or sub-word to a numerical value. There are three main levels of tokenization:\n",
    "\n",
    "1. Using word-level tokenization with the sentence \"I love TensorFlow\" might result in \"I\" being 0, \"love\" being 1 and \"TensorFlow\" being 2. In this case, every word in a sequence considered a single token.\n",
    "\n",
    "2. Character-level tokenization, such as converting the letters A-Z to values 1-26. In this case, every character in a sequence considered a single token.\n",
    "\n",
    "3. Sub-word tokenization is in between word-level and character-level tokenization. It involves breaking individual words into smaller parts and then converting those smaller parts into numbers. For example, \"my favorite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple tokens.\n",
    "\n",
    "* Embeddings - An embedding is a representation of natural language which can be learned. Representation comes in the form of a feature vector. For example, the word \"dance\" could be represented by the 5-dimensional vector [-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings:\n",
    "\n",
    "1. Create your own embedding - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as tf.keras.layers.Embedding) and an embedding representation will be learned during model training.\n",
    "\n",
    "2. Reuse a pre-learned embedding - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting text into numbers\n",
    "\n",
    "When dealing with a text problem, one of the first things you'll have to do before you can build a model is to convert your text to numbers.\n",
    "\n",
    "There are a few ways to do this, namely:\n",
    "* Tokenization - Direct mapping of token (a token could be a word or a character) to number.\n",
    "* Embedding - Create a matrix of feature vector for each token (the size of the feature vector can be defined and this embedding can be learned)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization (tokenization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The processing of each sample contains the following steps:\n",
    "1. standardize each sample (usually lowercasing + punctuation stripping)\n",
    "2. split each sample into substrings (usually words)\n",
    "3. recombine substrings into tokens (usually ngrams) (ngrams is group of words)\n",
    "4. index tokens (associate a uniquint value with each token)\n",
    "5. transform each sample using this index, either into a vector of ints or a dense float vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text vectorization (tokenization)\n",
    "Enough talking about tokenization and embeddings, let's create some.\n",
    "\n",
    "We'll practice tokenzation (mapping our words to numbers) first.\n",
    "\n",
    "To tokenize our words, we'll use the helpful preprocessing layer tf.keras.layers.experimental.preprocessing.TextVectorization.\n",
    "\n",
    "The TextVectorization layer takes the following parameters:\n",
    "* max_tokens - The maximum number of words in your vocabulary (e.g. 20000 or the number of unique words in your text), includes a value for OOV (out of vocabulary) tokens.\n",
    "* standardize - Method for standardizing text. Default is \"lower_and_strip_punctuation\" which lowers text and removes all punctuation marks.\n",
    "* split - How to split text, default is \"whitespace\" which splits on spaces.\n",
    "* ngrams - How many words to contain per token split, for example, ngrams=2 splits tokens into continuous sequences of 2.\n",
    "* output_mode - How to output tokens, can be \"int\" (integer mapping), \"binary\" (one-hot encoding), \"count\" or \"tf-idf\". See documentation for more.\n",
    "* output_sequence_length - Length of tokenized sequence to output. For example, if output_sequence_length=150, all tokenized sequences will be 150 tokens long.\n",
    "* pad_to_max_tokens - Defaults to False, if True, the output feature axis will be padded to max_tokens even if the number of unique tokens in the vocabulary is less than max_tokens. Only valid in certain modes, see docs for more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default TextVectorization variables\n",
    "text_vectorizer = TextVectorization(max_tokens=None, # how many words in the vocabulary (all of the different words in your text)\n",
    "                                    standardize=\"lower_and_strip_punctuation\", # how to process text\n",
    "                                    split=\"whitespace\", # how to split tokens\n",
    "                                    ngrams=None, # create groups of n-words?\n",
    "                                    output_mode=\"int\", # how to map tokens to numbers\n",
    "                                    output_sequence_length=None) # how long should the output sequence of tokens be?\n",
    "                                    # pad_to_max_tokens=True) # Not valid if using max_tokens=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@mogacola', '@zamtriossu', 'i', 'screamed', 'after', 'hitting', 'tweet']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the average number of tokens (words) in the training tweets\n",
    "round(sum([len(i.split()) for i in train_sentences]) / len(train_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup text vectorization with custom variables\n",
    "max_vocab_length = 10000 # max number of words to have in our vocabulary\n",
    "max_length = 15 # max length our sequences will be (e.g. how many words from a Tweet does our model see?)\n",
    "\n",
    "text_vectorizer = TextVectorization(max_tokens = max_vocab_length,\n",
    "                                    output_mode = \"int\",\n",
    "                                    output_sequence_length = max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the text vectorizer to the training data\n",
    "# adapt : Fits the state of the preprocessing layer to the dataset\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[264,   3, 232,   4,  13, 698,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=int64)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sample sentence and tokenize it \n",
    "sample_sentence = \"There's a flood in my street!\"\n",
    "text_vectorizer([sample_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " 70 years after #ABomb destroyd #HiroshimaÛÓ#BBC looks at wht #survived http://t.co/dLgNUuuUYn #CNV Watch Peace Vigils: http://t.co/jvkYzNDtja    \n",
      "\n",
      "Vectorized version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
       "array([[ 325,  141,   43, 6284,    1,    1,  287,   17, 3250,  363,    1,\n",
       "           1,  135,  675, 7026]], dtype=int64)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose a random sentence from the training dataset and tokenize it\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f\"Original text:\\n {random_sentence}\\\n",
    "    \\n\\nVectorized version:\")\n",
    "text_vectorizer([random_sentence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in vocab: 10000\n",
      "5 most common words: ['', '[UNK]', 'the', 'a', 'in']\n",
      "5 least common words: ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1']\n"
     ]
    }
   ],
   "source": [
    "# Get the unique words in the vocabulary\n",
    "words_in_vocab = text_vectorizer.get_vocabulary() # Get all the unique words \n",
    "top_5_words = words_in_vocab[:5] # get the most common words\n",
    "bottom_5_words= words_in_vocab[-5:] # get the least common words\n",
    "print(f\"Number of words in vocab: {len(words_in_vocab)}\")\n",
    "print(f\"5 most common words: {top_5_words}\")\n",
    "print(f\"5 least common words: {bottom_5_words}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Embedding using an Embedding Layer\n",
    "* Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "\n",
    "<https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding>\n",
    "\n",
    "The parameters we care most about for our embedding layer:\n",
    "* input_dim = the size of our vocabulary\n",
    "* output_dim = the size of the output embedding vector, for example, a value of 100 would be 100 long  \n",
    "* input_length = length of the sequences being passed to the embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.embeddings.Embedding at 0x2e2e5458d90>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Embedding\n",
    "embedding = layers.Embedding(input_dim = max_vocab_length, # set input shape\n",
    "                             embeddings_initializer = \"uniform\",\n",
    "                             output_dim = 128,\n",
    "                             input_length = max_length # how long is each input\n",
    "                             )\n",
    "\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      " It's not a cute dinner date Til cams nose starts bleeding    \n",
      "\n",
      "Embedded version:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
       "array([[[ 0.02485852,  0.03233856, -0.04716928, ...,  0.04837019,\n",
       "          0.0213588 , -0.01183138],\n",
       "        [-0.01495129, -0.01103653, -0.03942865, ..., -0.02537896,\n",
       "         -0.02434093, -0.02711899],\n",
       "        [-0.0443208 , -0.03886548,  0.01951635, ...,  0.0177494 ,\n",
       "         -0.01526488,  0.02859325],\n",
       "        ...,\n",
       "        [ 0.00250883, -0.04312918,  0.03395155, ..., -0.00681384,\n",
       "          0.02685476, -0.03502151],\n",
       "        [ 0.00250883, -0.04312918,  0.03395155, ..., -0.00681384,\n",
       "          0.02685476, -0.03502151],\n",
       "        [ 0.00250883, -0.04312918,  0.03395155, ..., -0.00681384,\n",
       "          0.02685476, -0.03502151]]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a random sentence from the training set\n",
    "random_sentence = random.choice(train_sentences)\n",
    "print(f'Original text:\\n {random_sentence}\\\n",
    "    \\n\\nEmbedded version:')\n",
    "\n",
    "# Embed the random sentence (turn it into dense vectors of fixed size)\n",
    "sample_embed = embedding(text_vectorizer([random_sentence]))\n",
    "sample_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(128,), dtype=float32, numpy=\n",
       " array([ 0.02485852,  0.03233856, -0.04716928,  0.03723738, -0.01368067,\n",
       "        -0.02831901, -0.0383569 , -0.01348202, -0.04510629,  0.03319595,\n",
       "         0.02417148,  0.02111593, -0.02403777,  0.02472608, -0.04178238,\n",
       "        -0.02655692,  0.04049946, -0.0412048 , -0.04689313,  0.03895967,\n",
       "         0.04284109, -0.01429711,  0.00191595, -0.03761158, -0.03745779,\n",
       "        -0.00706595, -0.01503211,  0.01544204,  0.0246747 , -0.02154256,\n",
       "        -0.02964962,  0.03123445,  0.04383769, -0.01013821, -0.02994606,\n",
       "        -0.04985712, -0.00602702, -0.00833012, -0.03166606,  0.01255685,\n",
       "         0.00366644, -0.00241054, -0.04014023, -0.03372522,  0.03008522,\n",
       "        -0.03119853, -0.04816645, -0.04837053,  0.04360266,  0.02849423,\n",
       "        -0.04630911,  0.01529081,  0.01092824, -0.04813163, -0.04680904,\n",
       "         0.03699395,  0.00532144,  0.04095611,  0.02114103,  0.04271544,\n",
       "        -0.04360133, -0.04761641, -0.0201099 , -0.00509583, -0.01948512,\n",
       "         0.04096562, -0.04522369, -0.04277518,  0.01243734, -0.03846997,\n",
       "        -0.00646392, -0.04285107, -0.0428847 ,  0.03757347,  0.04741028,\n",
       "        -0.02278719,  0.03875137, -0.02018541, -0.04322561,  0.0076599 ,\n",
       "         0.00028014, -0.0354314 ,  0.01777652, -0.04162989,  0.01614283,\n",
       "        -0.04397736, -0.03438614, -0.02104689, -0.0310493 ,  0.00293723,\n",
       "        -0.02237498,  0.04875317, -0.02560171,  0.04668791,  0.00514544,\n",
       "        -0.04680258, -0.00369338, -0.01316601, -0.02490356,  0.00633583,\n",
       "        -0.03978506,  0.00648708,  0.02901449, -0.00463538, -0.02626627,\n",
       "        -0.01041539,  0.04852661, -0.02713035, -0.03088059,  0.0191949 ,\n",
       "         0.04693625,  0.04593018,  0.03779826,  0.03558067, -0.047849  ,\n",
       "         0.01975377,  0.02930586, -0.04831895,  0.01218592,  0.00142784,\n",
       "        -0.03759914,  0.01833816, -0.00147809,  0.02457844, -0.04480967,\n",
       "         0.04837019,  0.0213588 , -0.01183138], dtype=float32)>,\n",
       " TensorShape([128]),\n",
       " \"It's not a cute dinner date Til cams nose starts bleeding\")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out a single token's embedding\n",
    "sample_embed[0][0] , sample_embed[0][0].shape , random_sentence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments we're running \n",
    "* Model 0: Naive Bayes with TF-IDF encoder (baseline)\n",
    "* Model 1: Feed-forward neural network (dense model)\n",
    "* Model 2: LSTM (RNN)\n",
    "* Model 3: GRU (RNN)\n",
    "* Model 4: Bidirectional-LSTM (RNN)\n",
    "* Model 5: 1D Convolutional Neural Network\n",
    "* Model 6: TensorFlow Hub Pretrained Feature Extractor\n",
    "* Model 7: TensorFlow Hub Pretrained Feature Extractor\n",
    "(10% of data) \n",
    "\n",
    "How are we going to approach all of these?\n",
    "USe the standard steps in modelling with tensorflow\n",
    "* Create a model\n",
    "* Build a model\n",
    "* Fit a model\n",
    "* Evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0: Getting a baseline\n",
    "\n",
    "As with all machine learning modelling experiments, it's important to create a baseline model so you've got a benchmark for future experiments to build upon.\n",
    "\n",
    "To create our baseline, we'll use SkLearn's Multi-nomial Naive Bayes using the TF-IDF formula to convert our words to numbers.\n",
    "\n",
    "> **Note:** It's a common practice to use non-DL algorithm as a baseline because of their speed, and then later using DL to see if you can improve upon them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer()), ('clf', MultinomialNB())])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create tokenization and modelling pipeline\n",
    "# TF-IDF (term frequency-inverse document frequency) is a statistical measure,\n",
    "# that evaluates how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "# This is done by multiplying two metrics: how many times a word appears in a document,\n",
    "# and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "model_0 = Pipeline([\n",
    "    (\"tfidf\" , TfidfVectorizer()), # Convert words to numbers using tfidf \n",
    "    (\"clf\" , MultinomialNB()) # Model the text and \"clf\" stands for classifier\n",
    "])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "model_0.fit(train_sentences , train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our baseline model achieves an accuracy of: 79.27%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our baseline model\n",
    "baseline_score = model_0.score(val_sentences , val_labels)\n",
    "print(f\"Our baseline model achieves an accuracy of: {baseline_score*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "baseline_preds = model_0.predict(val_sentences)\n",
    "baseline_preds[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an evaluation function for our model experiments\n",
    "\n",
    "We could evaluate all of our model's predictions with different metrics every time, however, this will be cumbersome and could  easily be fixed with a function.\n",
    "\n",
    "Let's create one to compare our model's predictions with the truth labels using the following metrics:\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate: accuracy, precision, recall, f1-score\n",
    "from sklearn.metrics import accuracy_score , precision_recall_fscore_support\n",
    " \n",
    "def calculate_results(y_true , y_pred):\n",
    "    \"\"\"\n",
    "    Calculate model accuracy, precision, recall and f1 score of a binary classification model.\n",
    "    \"\"\"\n",
    "    # Calculate model accuracy\n",
    "    model_accuracy = accuracy_score(y_true , y_pred) * 100\n",
    "    # Calculate model precision, recall and f1-score using \"weighted\" average\n",
    "    model_precision, model_recall , model_f1 , _ = precision_recall_fscore_support(y_true , y_pred, average = 'weighted')\n",
    "    model_results = {\"accuracy\": model_accuracy,\n",
    "                     \"precision\" : model_precision,\n",
    "                     \"recall\" : model_recall,\n",
    "                     \"f1\" : model_f1}\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get baseline results \n",
    "baseline_results = calculate_results(y_true = val_labels,\n",
    "                                     y_pred = baseline_preds)\n",
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: A simple dense model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensorboard callback (need to create a new one for each model)\n",
    "from helper_functions import create_tensorboard_callback\n",
    "\n",
    "# Create a directory to save TensorBoard logs\n",
    "SAVE_DIR = \"model_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model with the Functional API\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape=(1,), dtype=\"string\") # inputs are 1-dimensional strings\n",
    "x = text_vectorizer(inputs) # turn the input text into numbers\n",
    "x = embedding(x) # create an embedding of the numerized numbers\n",
    "x = layers.GlobalMaxPooling1D()(x) # lower the dimensionality of the embedding (try running the model without this layer and see what happens)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer, want binary outputs so use sigmoid activation\n",
    "model_1 = tf.keras.Model(inputs, outputs, name=\"model_1_dense\") # construct the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model_1.compile(loss=\"binary_crossentropy\",\n",
    "                optimizer=tf.keras.optimizers.Adam(),\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 14s 25ms/step - loss: 0.6424 - accuracy: 0.6487 - val_loss: 0.5788 - val_accuracy: 0.7585\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 4s 20ms/step - loss: 0.4743 - accuracy: 0.8323 - val_loss: 0.4755 - val_accuracy: 0.7848\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.3419 - accuracy: 0.8743 - val_loss: 0.4528 - val_accuracy: 0.7900\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 6s 27ms/step - loss: 0.2582 - accuracy: 0.9072 - val_loss: 0.4598 - val_accuracy: 0.7874\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 6s 28ms/step - loss: 0.1979 - accuracy: 0.9352 - val_loss: 0.4697 - val_accuracy: 0.7913\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_1_history = model_1.fit(train_sentences, # input sentences can be a list of strings due to text preprocessing layer built-in model\n",
    "                              train_labels,\n",
    "                              epochs=5,\n",
    "                              validation_data=(val_sentences, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 5ms/step - loss: 0.4697 - accuracy: 0.7913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.46970465779304504, 0.7913385629653931]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.evaluate(val_sentences , val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762, 1)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make some predictions and evaluate those\n",
    "model_1_pred_probs = model_1.predict(val_sentences)\n",
    "model_1_pred_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model prediction probabilities to label format\n",
    "model_1_preds = tf.squeeze(tf.round(model_1_pred_probs)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.13385826771653,\n",
       " 'precision': 0.7957855407433384,\n",
       " 'recall': 0.7913385826771654,\n",
       " 'f1': 0.7886149964743017}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate our model 1 results\n",
    "model_1_results = calculate_results(y_true = val_labels,\n",
    "                                    y_pred = model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing learned embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, ['', '[UNK]', 'the', 'a', 'in', 'to', 'of', 'and', 'i', 'is'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the vocabulary from the text vectorizattion layer\n",
    "words_in_vocab = text_vectorizer.get_vocabulary()\n",
    "len(words_in_vocab) , words_in_vocab[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_vocab_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1_dense\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,280,129\n",
      "Trainable params: 1,280,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.03737159, -0.09939637,  0.09148011, ..., -0.0810068 ,\n",
       "         -0.03535755, -0.03866373],\n",
       "        [-0.03986329, -0.0303731 , -0.04581859, ..., -0.05121184,\n",
       "         -0.03792265, -0.04977376],\n",
       "        [-0.02781821, -0.06172787, -0.01588579, ..., -0.0306049 ,\n",
       "         -0.04177805, -0.01413913],\n",
       "        ...,\n",
       "        [ 0.02937244,  0.00391371,  0.01603809, ...,  0.01262745,\n",
       "          0.03735823, -0.04100728],\n",
       "        [-0.0409763 , -0.01351456,  0.02660655, ..., -0.02940427,\n",
       "         -0.03146774, -0.03172175],\n",
       "        [-0.02211489, -0.00451796,  0.04047531, ..., -0.01452338,\n",
       "         -0.01123087, -0.04482847]], dtype=float32)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the weight matrix of embedding layer\n",
    "# (these are the numerical representations of each token in our training data,\n",
    "# which have been learned for -5 epochs)\n",
    "\n",
    "embed_weights = model_1.get_layer('embedding').get_weights()\n",
    "embed_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got the embedding matrix our model has learned to represent our tokens, let's see how we can visualize it.\n",
    "\n",
    "To do so, TensorFlow has a handy tool called projector: <http://projector.tensorflow.org/>\n",
    "\n",
    "And TensorFlow also has an incredible guide on word embeddings themselves: <https://www.tensorflow.org/tutorials/text/word_embeddings>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = model_1.get_layer('embedding').get_weights()[0]\n",
    "vocab = text_vectorizer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding files (we got this from TensorFlow's word embeddings documentation)\n",
    "import io\n",
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files from Colab to upload to projector\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('vector.tsv')\n",
    "    files.download('metadata.tsv')\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading the files above we can visualize them using <https://projector.tensorflow.org/>\n",
    "\n",
    "    Resources: If you'd like to know more about embeddings, check out:\n",
    "* Jay Alammar's visualized word2vec post: <https:jalammar.github.io/illustrated-word2vec/>\n",
    "* TensorFlow's Word Embeddings guide: <https://www.tensorflow.org/tutorials/text/word_embeddings> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Networks (RNN's)\n",
    "\n",
    "RNN's are useful for sequence data.\n",
    "\n",
    "The premise of a recurrent neural network is to use the representation of a previous input to aid the representation of a later input.\n",
    "\n",
    "IF you want an overview of the internals of a recurrent neural network, see the following:\n",
    "- MIT's sequence modelling lecture: <https://youtu.be/qjrad0V0uJE>\n",
    "- Chris Olah's intro to LSTMs: <https://colah.github.io/posts/2015-08-Understanding-LSTMs/>\n",
    "- Andrej Karpathy's the unreasonable effectiveness of recurrent neural networks: <http://karpathy.github.io/2015/05/21/rnn-effectiveness/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: LSTM \n",
    "LSTM = long short term memory (one of the most popular LSTM cells)\n",
    "Our structure of an RNN typically looks like this:\n",
    "\n",
    "Input (Text) -> Tokenize -> Embedding -> Layers (RNNs/dense) -> Output (Label probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 15, 128)\n",
      "(None, 15, 64)\n",
      "(None, 64)\n"
     ]
    }
   ],
   "source": [
    "# Create an LSTM model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import LSTM\n",
    "inputs = layers.Input(shape = (1 , ) , dtype = tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "print(x.shape)\n",
    "x = LSTM(units = 64 , return_sequences = True)(x) # When you're stacking RNN cells together, you need to return_sequences = True\n",
    "print(x.shape)\n",
    "x = layers.LSTM(64)(x)\n",
    "print(x.shape)\n",
    "x = layers.Dense(64 , activation = 'relu')(x)\n",
    "outputs = layers.Dense(1 , activation = 'sigmoid')(x)\n",
    "\n",
    "model_2 = tf.keras.Model(inputs , outputs , name = 'model_2_LSTM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2_LSTM\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 15, 64)            49408     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,366,657\n",
      "Trainable params: 1,366,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_2.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = tf.keras.optimizers.Adam(),\n",
    "                metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 32s 66ms/step - loss: 0.3776 - accuracy: 0.8374 - val_loss: 0.4848 - val_accuracy: 0.7703\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 9s 44ms/step - loss: 0.2384 - accuracy: 0.9061 - val_loss: 0.5342 - val_accuracy: 0.7927\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 7s 33ms/step - loss: 0.1749 - accuracy: 0.9321 - val_loss: 0.5994 - val_accuracy: 0.7769\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 8s 36ms/step - loss: 0.1252 - accuracy: 0.9488 - val_loss: 0.8548 - val_accuracy: 0.7651\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 9s 42ms/step - loss: 0.0921 - accuracy: 0.9634 - val_loss: 0.9061 - val_accuracy: 0.7664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e2e6b0acd0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_2.fit(x = train_sentences,\n",
    "            y = train_labels,\n",
    "            epochs = 5,\n",
    "            validation_data = (val_sentences , val_labels)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.32794964],\n",
       "       [0.32591593],\n",
       "       [0.99734676],\n",
       "       [0.15124673],\n",
       "       [0.00184265],\n",
       "       [0.99999446],\n",
       "       [0.8675568 ],\n",
       "       [0.99999595],\n",
       "       [0.99999   ],\n",
       "       [0.50123966]], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with LSTM model\n",
    "model_2_pred_probs = model_2.predict(val_sentences)\n",
    "model_2_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 1., 0., 0., 1., 1., 1., 1., 1.], dtype=float32)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert model 2 pred probs to labels \n",
    "model_2_preds = tf.squeeze(tf.round(model_2_pred_probs))\n",
    "model_2_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.64041994750657,\n",
       " 'precision': 0.7661818147921768,\n",
       " 'recall': 0.7664041994750657,\n",
       " 'f1': 0.7655229275917975}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model 2 results\n",
    "model_2_results = calculate_results(y_true = val_labels,\n",
    "                                    y_pred = model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: GRU\n",
    "\n",
    "Another popular and effective RNN component is the GRU or gated recurrent unit.\n",
    "\n",
    "The GRU cell has similar features to an LSTM cell but has less parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an RNN using the GRU cell\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape = (1, ) , dtype = tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.GRU(128 , return_sequences = True)(x)\n",
    "x = layers.LSTM(512 , return_sequences = True)(x)\n",
    "x = layers.GRU(1024 , return_sequences = True)(x)\n",
    "x = layers.MaxPool1D()(x)\n",
    "x = layers.Dense(256 , activation = 'LeakyReLU')(x)      \n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "outputs = layers.Dense(1 , activation = 'sigmoid')(x)\n",
    "model_3 = tf.keras.Model(inputs , outputs , name = 'model_3_GRU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3_GRU\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 15, 128)           99072     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 15, 512)           1312768   \n",
      "                                                                 \n",
      " gru_1 (GRU)                 (None, 15, 1024)          4724736   \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 7, 1024)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 7, 256)            262400    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 256)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,679,233\n",
      "Trainable params: 7,679,233\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_3.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = tf.keras.optimizers.Adam(),\n",
    "                metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 178s 768ms/step - loss: 0.1646 - accuracy: 0.9403 - val_loss: 0.6457 - val_accuracy: 0.7677\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 176s 817ms/step - loss: 0.1084 - accuracy: 0.9622 - val_loss: 0.6152 - val_accuracy: 0.7336\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 185s 861ms/step - loss: 0.0893 - accuracy: 0.9658 - val_loss: 1.2152 - val_accuracy: 0.7638\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 186s 864ms/step - loss: 0.0697 - accuracy: 0.9669 - val_loss: 1.2499 - val_accuracy: 0.7690\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 185s 860ms/step - loss: 0.0922 - accuracy: 0.9628 - val_loss: 1.5638 - val_accuracy: 0.7769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e2e8e3c610>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_3.fit(x = train_sentences,\n",
    "            y = train_labels,\n",
    "            epochs = 5,\n",
    "            validation_data = (val_sentences , val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.4908960e-02],\n",
       "       [6.3286465e-01],\n",
       "       [9.9666882e-01],\n",
       "       [5.3191483e-03],\n",
       "       [2.0460395e-06],\n",
       "       [9.9933851e-01],\n",
       "       [6.4136481e-01],\n",
       "       [9.9999905e-01],\n",
       "       [9.9993122e-01],\n",
       "       [2.9470533e-02]], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make some predictions with our GRU model\n",
    "model_3_pred_probs = model_3.predict(val_sentences)\n",
    "model_3_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert model 3 pred probs to labels\n",
    "model_3_preds = tf.squeeze(tf.round(model_3_pred_probs))\n",
    "model_3_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 77.69028871391076,\n",
       " 'precision': 0.7822241302284023,\n",
       " 'recall': 0.7769028871391076,\n",
       " 'f1': 0.7734519762210931}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3_results = calculate_results(y_true = val_labels,\n",
    "                                    y_pred = model_3_preds)\n",
    "model_3_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 79.26509186351706,\n",
       " 'precision': 0.8111390004213173,\n",
       " 'recall': 0.7926509186351706,\n",
       " 'f1': 0.7862189758049549}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4: Bidectional RNN\n",
    "Normal RNN's go from left to right (just like you'd read an English sentence) however, a bidrectional RNN goes from right to left as well as left to right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a bidirectional RNN in TensorFlow\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape = (1 , ) , dtype = tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Bidirectional(layers.GRU(128 , return_sequences = True))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences = True))(x)\n",
    "x = layers.GlobalMaxPool1D()(x)\n",
    "outputs = layers.Dense(1 , activation = 'sigmoid')(x)\n",
    "model_4 = tf.keras.Model(inputs , outputs , name = 'model_4_bidirecttional')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4_bidirecttional\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 15, 256)          198144    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 15, 128)          164352    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Glo  (None, 128)              0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,642,625\n",
      "Trainable params: 1,642,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_4.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = tf.keras.optimizers.Adam(),\n",
    "                metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 32s 94ms/step - loss: 0.1297 - accuracy: 0.9540 - val_loss: 0.8553 - val_accuracy: 0.7677\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 17s 78ms/step - loss: 0.0665 - accuracy: 0.9721 - val_loss: 1.1123 - val_accuracy: 0.7717\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 17s 78ms/step - loss: 0.0535 - accuracy: 0.9759 - val_loss: 1.0613 - val_accuracy: 0.7756\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 17s 78ms/step - loss: 0.0461 - accuracy: 0.9793 - val_loss: 1.2714 - val_accuracy: 0.7507\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 17s 79ms/step - loss: 0.0421 - accuracy: 0.9797 - val_loss: 1.4447 - val_accuracy: 0.7690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e284458820>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_4.fit(x = train_sentences,\n",
    "            y = train_labels,\n",
    "            epochs = 5,\n",
    "            validation_data = (val_sentences , val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.4345517e-04],\n",
       "       [7.3600268e-01],\n",
       "       [9.9991959e-01],\n",
       "       [1.3414189e-01],\n",
       "       [8.5497908e-05],\n",
       "       [9.9929065e-01],\n",
       "       [8.9634168e-01],\n",
       "       [9.9992436e-01],\n",
       "       [9.9992985e-01],\n",
       "       [3.6375433e-01]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pred probs to pred labels\n",
    "model_4_pred_probs = model_4.predict(val_sentences)\n",
    "model_4_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert pred probs to pred labels \n",
    "model_4_preds = tf.squeeze(tf.round(model_4_pred_probs))\n",
    "model_4_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.9028871391076,\n",
       " 'precision': 0.7708613696015271,\n",
       " 'recall': 0.7690288713910761,\n",
       " 'f1': 0.766802361272573}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the results of our bidrectional model\n",
    "model_4_results = calculate_results(y_true = val_labels,\n",
    "                                    y_pred = model_4_preds)\n",
    "model_4_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution Neural Networks for Text (and other types of sequences)\n",
    "\n",
    "We've used CNNs for images but images are typically 2D (height x width)... however, out text data is 1D.\n",
    "\n",
    "Previously we've Conv2D for our image data but now we're going to use Conv1D.\n",
    "\n",
    "The typical structure of a Conv1D model for sequences (in our case, text):\n",
    "\n",
    "```\n",
    "Inputs (text) -> Tokenization -> Embedding -> Layer(s)\n",
    "(typically Conv1D + pooling) -> Outputs (class probabilities)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5: Conv1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([1, 15, 128]), TensorShape([1, 11, 32]), TensorShape([1, 32]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test out out embedding layer, Conv1D layer and max pooling\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "embedding_test = embedding(text_vectorizer([\"this is a test sentence\"])) # Turn target sequence into embedding\n",
    "conv_1d = Conv1D(filters = 32,\n",
    "                        kernel_size = 5, # this is also referred to as an ngram of 5 (meaning it looks at 5 words at a time)\n",
    "                        activation = 'LeakyReLU',\n",
    "                        padding = 'valid') # default = 'valid', the output is smaller than the input shape, 'same' means output is same shape as input\n",
    "conv_1d_output = conv_1d(embedding_test) # pass test embedding through conv1d layer\n",
    "max_pool = layers.GlobalMaxPool1D()\n",
    "max_pool_output = max_pool(conv_1d_output) # equivalent to \"get the most important feature\" or \"get the feature with the highest value\"\n",
    "embedding_test.shape , conv_1d_output.shape , max_pool_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 1-dimensional convolutional layer to model sequences\n",
    "from tensorflow.keras import layers\n",
    "inputs = layers.Input(shape = (1 , ) , dtype = tf.string)\n",
    "x = text_vectorizer(inputs)\n",
    "x = embedding(x)\n",
    "x = layers.Conv1D(filters = 64, kernel_size = 3 , activation = 'LeakyReLU', strides = 1, padding = 'same')(x)\n",
    "x = layers.GlobalMaxPool1D()(x) \n",
    "x = layers.Dense(64 , activation = 'LeakyReLU')(x)\n",
    "outputs = layers.Dense(1 , activation = 'sigmoid')(x)\n",
    "\n",
    "model_5 = tf.keras.Model(inputs , outputs , name = 'model_5_Conv1D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5_Conv1D\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_1 (TextV  (None, 15)               0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 15, 128)           1280000   \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 15, 64)            24640     \n",
      "                                                                 \n",
      " global_max_pooling1d_3 (Glo  (None, 64)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 64)                4160      \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,308,865\n",
      "Trainable params: 1,308,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model_5.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = tf.keras.optimizers.Adam(),\n",
    "                metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 14s 44ms/step - loss: 0.1399 - accuracy: 0.9517 - val_loss: 0.8864 - val_accuracy: 0.7559\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.0664 - accuracy: 0.9737 - val_loss: 0.9809 - val_accuracy: 0.7690\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.0536 - accuracy: 0.9791 - val_loss: 1.1294 - val_accuracy: 0.7533\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 5s 25ms/step - loss: 0.0444 - accuracy: 0.9806 - val_loss: 1.1497 - val_accuracy: 0.7572\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 5s 24ms/step - loss: 0.0380 - accuracy: 0.9825 - val_loss: 1.3432 - val_accuracy: 0.7638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2e28497dd60>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model_5.fit(x = train_sentences,\n",
    "            y = train_labels,\n",
    "            epochs = 5,\n",
    "            validation_data = (val_sentences , val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.4117351e-02],\n",
       "       [7.0982897e-01],\n",
       "       [9.9984968e-01],\n",
       "       [6.5873057e-02],\n",
       "       [2.5609457e-05],\n",
       "       [9.9230397e-01],\n",
       "       [9.9045825e-01],\n",
       "       [9.9989414e-01],\n",
       "       [9.9998546e-01],\n",
       "       [1.5256831e-01]], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5_pred_probs = model_5.predict(val_sentences)\n",
    "model_5_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert model 5 pred probs to labels\n",
    "model_5_preds = tf.squeeze(tf.round(model_5_pred_probs))\n",
    "model_5_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 76.37795275590551,\n",
       " 'precision': 0.765729922717656,\n",
       " 'recall': 0.7637795275590551,\n",
       " 'f1': 0.7613639638656104}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate model 5 predictions\n",
    "model_5_results = calculate_results(y_true = val_labels,\n",
    "                                    y_pred = model_5_preds)\n",
    "model_5_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **USE-** Universal Feature Extractor\n",
    "Source: <https://tfhub.dev/google/universal-sentence-encoder/4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6: TensorFlow Hub Pretrained Sentence Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"There's a flood in my street!\""
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[-0.01157027  0.02485911  0.02878048 -0.01271501  0.03971541  0.08827761\n",
      "  0.02680985  0.05589836 -0.01068731 -0.00597293  0.00639323 -0.01819516\n",
      "  0.00030815  0.09105889  0.05874644 -0.03180626  0.01512474 -0.05162926\n",
      "  0.00991366 -0.06865344 -0.04209306  0.02678978  0.03011006  0.00321068\n",
      " -0.00337968 -0.04787356  0.0226672  -0.00985928 -0.04063615 -0.01292093\n",
      " -0.04666383  0.056303   -0.03949254  0.00517684  0.02495828 -0.0701444\n",
      "  0.0287151   0.04947681 -0.00633977 -0.08960192  0.0280712  -0.00808363\n",
      " -0.01360601  0.0599865  -0.10361788 -0.05195374  0.00232955 -0.02332529\n",
      " -0.03758105  0.03327728], shape=(50,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "embed_samples = embed([sample_sentence,\n",
    "                      \"When you can the universal sentence encoder on a sentence, it turns it into numbers.\"])\n",
    "print(embed_samples[0][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 512])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Keras Layer using the USE pretrained layer from tensorflow hub\n",
    "sentence_encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
    "                                        input_shape = [],\n",
    "                                        dtype = tf.string,\n",
    "                                        trainable = False,\n",
    "                                        name = \"USE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "215/215 [==============================] - 58s 109ms/step - loss: 0.5036 - accuracy: 0.7824 - val_loss: 0.4503 - val_accuracy: 0.8071\n",
      "Epoch 2/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.4148 - accuracy: 0.8136 - val_loss: 0.4404 - val_accuracy: 0.8123\n",
      "Epoch 3/5\n",
      "215/215 [==============================] - 2s 10ms/step - loss: 0.4012 - accuracy: 0.8222 - val_loss: 0.4321 - val_accuracy: 0.8163\n",
      "Epoch 4/5\n",
      "215/215 [==============================] - 2s 9ms/step - loss: 0.3926 - accuracy: 0.8253 - val_loss: 0.4301 - val_accuracy: 0.8189\n",
      "Epoch 5/5\n",
      "215/215 [==============================] - 2s 8ms/step - loss: 0.3844 - accuracy: 0.8301 - val_loss: 0.4267 - val_accuracy: 0.8123\n"
     ]
    }
   ],
   "source": [
    "# Create model using the Sequential API\n",
    "model_6 = tf.keras.Sequential([\n",
    "    sentence_encoder_layer,\n",
    "    layers.Dense(64 , activation = 'relu'),\n",
    "    layers.Dense(1 , activation = 'sigmoid'),\n",
    "], name = 'model_6_USE')\n",
    "\n",
    "# Compile the model\n",
    "model_6.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = tf.keras.optimizers.Adam(),\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history_model_6 = model_6.fit(x = train_sentences,\n",
    "            y = train_labels,\n",
    "            epochs = 5,\n",
    "            validation_data = (val_sentences , val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.20312235],\n",
       "       [0.80665845],\n",
       "       [0.9886973 ],\n",
       "       [0.23531431],\n",
       "       [0.74456537],\n",
       "       [0.78820384],\n",
       "       [0.9805012 ],\n",
       "       [0.984424  ],\n",
       "       [0.94469553],\n",
       "       [0.11483777]], dtype=float32)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions with USE TF Hub Model\n",
    "model_6_pred_probs = model_6.predict(val_sentences)\n",
    "model_6_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 1., 0., 1., 1., 1., 1., 1., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert prediction probabilities to label\n",
    "model_6_preds = tf.squeeze(tf.round(model_6_pred_probs))\n",
    "model_6_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 81.23359580052494,\n",
       " 'precision': 0.8129238565064447,\n",
       " 'recall': 0.8123359580052494,\n",
       " 'f1': 0.8114314841586915}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate model 6 performance metrics\n",
    "model_6_results = calculate_results(y_true = val_labels,\n",
    "                                    y_pred = model_6_preds)\n",
    "model_6_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7: TF Hub Pretrained USE but with 10% of training data\n",
    "\n",
    "Transfer learning really helps when you don't have a large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_10_percent_split = int(0.1 * len(train_sentences))\n",
    "train_sentences_10_percent = train_sentences[: training_10_percent_split]\n",
    "train_labels_10_percent = train_labels[:training_10_percent_split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(np.array(train_labels_10_percent)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recreate a model the same as a previous model you've created you can use the `tf.keras.models.clone_model()` method\n",
    "* Model cloning is similar to calling a model on new inputs, except that it creates new layer (and thus new weights) instead of sharing the weights of the existing layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7 = tf.keras.models.clone_model(model_6 , name = \"model_7_USE\")\n",
    "\n",
    "# Compile the model\n",
    "model_7.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = tf.keras.optimizers.Adam(),\n",
    "                metrics = ['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history_model_7 = model_7.fit(x = train_sentences,\n",
    "            y = train_labels,\n",
    "            epochs = 5,\n",
    "            validation_data = (val_sentences , val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions with the model trained on 10% of the data\n",
    "model_7_pred_probs = model_7.predict(val_sentences)\n",
    "model_7_pred_probs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn pred probs into labels\n",
    "model_7_preds = tf.squeeze(tf.round(model_7_pred_probs))\n",
    "model_7_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_7_results = calculate_results(y_true = val_labels,\n",
    "                                    y_pred = model_7_preds)\n",
    "model_7_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the performance of each of our models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine model results into a DataFrame\n",
    "all_model_results = pd.DataFrame({'0_baseline': baseline_results,\n",
    "                                  '1_simple_dense': model_1_results,\n",
    "                                  '2_lstm': model_2_results,\n",
    "                                  '3_gru': model_3_results,\n",
    "                                  '4_bidirectional': model_4_results,\n",
    "                                  '5_conv1d': model_5_results,\n",
    "                                  '6_tf_hub_use_encoder': model_6_results,\n",
    "                                  '7_tf_hub__use_encoder': model_7_results})\n",
    "\n",
    "all_model_results = all_model_results.transpose()\n",
    "all_model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the accuracy to the same scale as other metrics\n",
    "all_model_results['accuracy'] = all_model_results['accuracy']/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and compare all of the model results\n",
    "all_model_results.plot(kind = 'bar' , figsize = (10 , 7).legend(bbox_to_anchor = (1.0 , 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort model results by f1-score\n",
    "all_model_results.sort_values('f1' , ascending = False)[\"f1\"].plot(kind = 'bar' , figsize = (10 , 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uploading our model training logs to TensoorBoard.dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and loading a trained model\n",
    "There are two main formats to save a model to in TensorFlow:\n",
    "1. The HDF5 format\n",
    "2. The `SavedModel` format (this is the default when using TensorFlow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TF Hub Sentence Encoder model to HDF5 format\n",
    "model_6.save(\"model_6.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with custom Hub Layer (required HDF5 format)\n",
    "import tensorflow_hub as hub\n",
    "loaded_model_6 = tf.keras.models.load_model(\"model_6.h5\",\n",
    "                                            custom_objects = {\"KerasLayer\": hub.KerasLayer})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's save to the SavedModel format...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save TF Hub Sentence Encoder model to SavedModel format\n",
    "model_6.save(\"model_6_SavedModel_format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a model from the SavedModel format\n",
    "loaded_model_6_SavedModel_format = tf.keras.models.load_model(\"model_6_SavedModel_format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most wrong examples\n",
    "\n",
    "* If our best model still isn't perfect, what examples is it getting wrong?\n",
    "* And of these wrong examples which ones is it getting *most* wrong (those will prediction probabilites closest to the opposite class)\n",
    "\n",
    "For examples if a samples should have a label of 0 but our model predicts a prediction probability of 0.999 (really close to 1) and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.DataFrame({'text': val_sentences,\n",
    "                       'target': val_labels,\n",
    "                       'pred': model_6_preds,\n",
    "                       'pred_probs': tf.squeeze(model_6_pred_probs)})\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the wrong predictions and sort by prediction probabilities\n",
    "most_wrong = val_df[val_df['target'] != val_df['pred']].sort_values('pred_probs' , asceding = False)\n",
    "most_wrong[:10]\n",
    "# These are false positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves of the target labels..\n",
    "* 0 = not disaster\n",
    "* 1 = disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_wrong.tail()\n",
    "# These are false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the false positives (model predicted 1 when should've been 0)\n",
    "for row in most_wrong[:10].itertuples():\n",
    "    _, text, target, pred, pred_prob = row\n",
    "    print(f\"Target: {target}, Pred: {pred}, Prob: {pred_prob}\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the false negatives (model predicted 0 when should've been 1)\n",
    "for row in most_wrong[-10:].itertuples():\n",
    "    _, text, target, pred, pred_prob = row\n",
    "    print(f\"Target: {target}, Pred: {pred}, Prob: {pred_prob}\")\n",
    "    print(f\"Text:\\n{text}\\n\")\n",
    "    print(\"----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making predictions on the test dataset and visualizing them\n",
    "test_sentences = test_df['text'].to_list()\n",
    "test_samples = random.sample(test_sentences , 10)\n",
    "for test_sample in test_samples:\n",
    "    pred_prob = tf.squeeze(model_6.predict(test_samples))\n",
    "    pred = tf.round(pred_prob)\n",
    "    print(f\"Pred: {int(pred)}, Prob: {pred_prob}\")\n",
    "    print(f\"Text:\\nn{test_sample}\\n\")\n",
    "    print(\"----\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The speed/score tradeoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a function to measure the time of prediction\n",
    "import time \n",
    "def pred_timer(model , samples):\n",
    "    '''\n",
    "    Times how long a model takes to make predictions on samples.\n",
    "    '''\n",
    "    start_time = time.perf_counter() #get start time \n",
    "    model.predict(samples) # Make predictions\n",
    "    end_time = time.perf_counter() # Get finish time \n",
    "    total_time = end_time - start_time # Calculate how long predictions took to make\n",
    "    time_per_pred = total_time / len(samples)\n",
    "    return time_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TF HUb Sentence Encoder time per pred\n",
    "model_6_total_pred_time , model_6_time_per_pred = pred_timer(model = model_6,\n",
    "                                                             samples = val_sentences)\n",
    "model_6_total_pred_time , model_6_time_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate TF HUb Sentence Encoder time per pred\n",
    "baseline_total_pred_time , baseline_time_per_pred = pred_timer(model = model_0,\n",
    "                                                             samples = val_sentences)\n",
    "baseline_total_pred_time , baseline_time_per_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure(figsize = (10 , 7))\n",
    "plt.scatter(baseline_time_per_pred , baseline_results['f1'] , label = 'baseline')\n",
    "plt.scatter(model_6_time_per_pred , model_6_results['f1'] , label = 'tf_hub')\n",
    "plt.legend()\n",
    "plt.title(\"F1-score versus time prediction\")\n",
    "plt.xlabel('Time per prediction')\n",
    "plt.ylabel(\"F1-score\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "83fdaa4a036c770b553794c5672bdb43ec32b098b80996ab992ce37e5ad2f3db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
